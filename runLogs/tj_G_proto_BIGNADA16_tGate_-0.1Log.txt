env action space is:  Discrete(2)
num inputs is:  17
[2]
[2, 2]
Namespace(num_epochs=2000, epoch_size=10, batch_size=500, nprocesses=16, hid_size=128, recurrent=True, gamma=1.0, tau=1.0, seed=20, normalize_rewards=False, lrate=0.001, entr=0, value_coeff=0.01, env_name='traffic_junction', max_steps=20, nactions='1', action_scale=1.0, plot=False, plot_env='main', save='trained_models', save_every=100, load='trained_models', display=False, random=False, commnet=1, ic3net=True, nagents=5, comm_mode='avg', comm_passes=1, comm_mask_zero=False, mean_ratio=0, rnn_type='LSTM', detach_gap=10, comm_init='uniform', hard_attn=1, comm_action_one=False, comm_action_zero=False, advantages_per_action=False, share_weights=False, log_dir='tb_logs', exp_name='tj_G_proto_BIGNADA16_tGate_-0.1', use_proto=True, discrete_comm=True, num_proto=25, comm_dim=16, gating_head_cost_factor=-0.1, restore=False, gate_reward_curriculum=False, gate_reward_max=-0.01, gate_reward_min=0.01, reward_curr_start=1500, reward_curr_end=1900, variable_gate=True, variable_gate_start=500, dim=6, vision=0, add_rate_min=0.1, add_rate_max=0.3, curr_start=250.0, curr_end=1250.0, difficulty='easy', vocab_type='bool', nfriendly=5, num_actions=[2, 2], dim_actions=2, num_inputs=17, continuous=False, naction_heads=[2, 2])
using commnet
====================================================================================================
Model log:

CommNetMLP(
  (proto_layer): ProtoNetwork(
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): Linear(in_features=64, out_features=25, bias=True)
    )
    (prototype_layer): ProtoLayer()
  )
  (heads): ModuleList(
    (0): Linear(in_features=128, out_features=2, bias=True)
    (1): Linear(in_features=128, out_features=2, bias=True)
  )
  (encoder): Linear(in_features=17, out_features=16, bias=True)
  (hidd_encoder): Linear(in_features=128, out_features=128, bias=True)
  (f_module): LSTMCell(16, 128)
  (C_modules): ModuleList(
    (0): Linear(in_features=16, out_features=16, bias=True)
  )
  (tanh): Tanh()
  (value_head): Linear(in_features=128, out_features=1, bias=True)
)
====================================================================================================

Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
Device: cpu
DO NOT RUN THIS: AUTO REWARD CURRICULUM IN BETA
save directory is trained_models/traffic_junction/tj_G_proto_BIGNADA16_tGate_-0.1/seed20/models
log dir directory is trained_models/traffic_junction/tj_G_proto_BIGNADA16_tGate_-0.1/seed20/logs
Epoch 0	Reward [-9.1  -9.45 -9.04 -9.14 -9.14]	Time 74.20s
Add-Rate: 0.10
Success: 0.50
Steps-taken: 20.00
Comm-Action: [0.51 0.51 0.51 0.51 0.51]
Epoch 1	Reward [-4.98 -5.23 -4.81 -5.09 -5.11]	Time 71.82s
Add-Rate: 0.10
Success: 0.67
Steps-taken: 20.00
Comm-Action: [0.48 0.48 0.49 0.48 0.48]
Epoch 2	Reward [-2.3  -2.11 -2.18 -2.48 -2.27]	Time 76.66s
Add-Rate: 0.10
Success: 0.82
Steps-taken: 20.00
Comm-Action: [0.55 0.54 0.54 0.55 0.55]
Epoch 3	Reward [-1.78 -1.84 -1.88 -1.85 -1.83]	Time 77.67s
Add-Rate: 0.10
Success: 0.83
Steps-taken: 20.00
Comm-Action: [0.58 0.57 0.58 0.57 0.58]
Epoch 4	Reward [-1.74 -1.62 -1.69 -1.66 -1.57]	Time 76.96s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.6  0.6  0.6  0.6  0.61]
Epoch 5	Reward [-1.61 -1.7  -1.68 -1.68 -1.67]	Time 74.86s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.62 0.61 0.61 0.61 0.62]
Epoch 6	Reward [-1.74 -1.76 -1.75 -1.67 -1.79]	Time 77.03s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.53 0.52 0.53 0.52 0.53]
Epoch 7	Reward [-1.61 -1.62 -1.67 -1.72 -1.72]	Time 76.53s
Add-Rate: 0.10
Success: 0.83
Steps-taken: 20.00
Comm-Action: [0.61 0.61 0.61 0.61 0.61]
Epoch 8	Reward [-1.36 -1.44 -1.47 -1.45 -1.4 ]	Time 74.27s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.72 0.71 0.71 0.71 0.71]
Epoch 9	Reward [-1.18 -1.3  -1.27 -1.26 -1.27]	Time 73.42s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.76 0.76 0.76 0.76 0.76]
Epoch 10	Reward [-1.32 -1.32 -1.38 -1.36 -1.31]	Time 72.00s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.75 0.75 0.75 0.75 0.75]
Epoch 11	Reward [-1.27 -1.24 -1.27 -1.15 -1.21]	Time 73.68s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.77 0.77 0.77 0.77 0.77]
Epoch 12	Reward [-1.45 -1.4  -1.43 -1.37 -1.34]	Time 72.91s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.71 0.71 0.71 0.71 0.71]
Epoch 13	Reward [-1.47 -1.55 -1.43 -1.47 -1.55]	Time 75.91s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.67 0.67 0.67 0.67 0.67]
Epoch 14	Reward [-1.36 -1.43 -1.34 -1.39 -1.44]	Time 75.74s
Add-Rate: 0.10
Success: 0.86
Steps-taken: 20.00
Comm-Action: [0.69 0.69 0.69 0.69 0.69]
Epoch 15	Reward [-1.42 -1.48 -1.5  -1.45 -1.46]	Time 75.22s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.66 0.66 0.66 0.66 0.66]
Epoch 16	Reward [-1.59 -1.54 -1.53 -1.55 -1.52]	Time 76.99s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.65 0.65 0.66 0.66 0.65]
Epoch 17	Reward [-1.51 -1.62 -1.55 -1.52 -1.57]	Time 76.99s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.63 0.63 0.63 0.62 0.62]
Epoch 18	Reward [-1.55 -1.53 -1.63 -1.62 -1.56]	Time 73.97s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.63 0.63 0.62 0.62 0.62]
Epoch 19	Reward [-1.52 -1.59 -1.57 -1.56 -1.55]	Time 70.03s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.64 0.64 0.64 0.64 0.64]
Epoch 20	Reward [-1.49 -1.49 -1.45 -1.44 -1.54]	Time 74.04s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.65 0.66 0.65 0.66 0.66]
Epoch 21	Reward [-1.45 -1.48 -1.46 -1.53 -1.44]	Time 80.49s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.65 0.65 0.66 0.65 0.65]
Epoch 22	Reward [-1.56 -1.62 -1.66 -1.61 -1.6 ]	Time 71.79s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.6 0.6 0.6 0.6 0.6]
Epoch 23	Reward [-1.75 -1.81 -1.74 -1.78 -1.74]	Time 72.57s
Add-Rate: 0.10
Success: 0.83
Steps-taken: 20.00
Comm-Action: [0.53 0.54 0.54 0.53 0.54]
Epoch 24	Reward [-1.87 -1.88 -1.86 -1.9  -1.9 ]	Time 71.52s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.45 0.45 0.46 0.46 0.45]
Epoch 25	Reward [-1.88 -1.92 -1.9  -1.85 -1.86]	Time 75.18s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.47 0.47 0.47 0.47 0.47]
Epoch 26	Reward [-2.09 -2.1  -2.11 -2.15 -2.12]	Time 70.93s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.34 0.34 0.34 0.34 0.34]
Epoch 27	Reward [-2.17 -2.18 -2.12 -2.15 -2.23]	Time 73.60s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.32 0.32 0.32 0.32 0.32]
Epoch 28	Reward [-2.2  -2.24 -2.16 -2.2  -2.21]	Time 71.51s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.3 0.3 0.3 0.3 0.3]
Epoch 29	Reward [-2.1  -2.11 -2.08 -2.21 -2.15]	Time 73.74s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.34 0.34 0.34 0.34 0.33]
Epoch 30	Reward [-1.9  -1.96 -1.9  -1.93 -1.96]	Time 74.75s
Add-Rate: 0.10
Success: 0.85
Steps-taken: 20.00
Comm-Action: [0.43 0.44 0.44 0.44 0.44]
Epoch 31	Reward [-1.65 -1.62 -1.64 -1.66 -1.67]	Time 63.09s
Add-Rate: 0.10
Success: 0.84
Steps-taken: 20.00
Comm-Action: [0.59 0.59 0.59 0.59 0.59]
